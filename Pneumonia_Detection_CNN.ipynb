{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j_mLVyoYWxw7"
      ],
      "mount_file_id": "1pmhjVIiEoKKekA_U0COO0JBuVV85zuPv",
      "authorship_tag": "ABX9TyOTVDYvG3A5t9KCuf26mM2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakarsh204/Pneumonia-Detection-CNN/blob/main/Pneumonia_Detection_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pneumonia Kaggle Classification"
      ],
      "metadata": {
        "id": "EfjFSgh3WmZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unzipping\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j_mLVyoYWxw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* zip file must be present in drive/MyDrive/< .zip file name >\n",
        "* zip file data extracted to drive/MyDrive/ML Data Sets/< folder name >\n",
        "* Checkpointing of models saved to drive/MyDrive/ML Models/< model name >\n",
        "* Train and Test history of models saved to drive/MyDrive/ML Models/Train and Test Data/< model name >"
      ],
      "metadata": {
        "id": "L29Eg1FlntLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Mount Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/chestxray.zip'\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zipObj:\n",
        "    zipObj.extractall('drive/MyDrive/ML Data Sets')"
      ],
      "metadata": {
        "id": "HpJjIEWv4fkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyper-Parameters"
      ],
      "metadata": {
        "id": "Hk0Xk_KHcbsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the following hyper-parameters:\n",
        "* Learning rate\n",
        "* Epochs\n",
        "* Batch Size\n",
        "* Patience\n",
        "* Resolution"
      ],
      "metadata": {
        "id": "xGBzALGvnIyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .zip file name\n",
        "ZIP_NAME = \"chestxray.zip\"\n",
        "FOLDER_NAME = \"Kaggle Chest X-Ray\"\n",
        "\n",
        "# Loading a Model and its Train and Test History\n",
        "load_status = True\n",
        "LOAD_NAME = \"CNN_v1\"\n",
        "\n",
        "# Saving a Model (state_dict only)\n",
        "save_status = True\n",
        "SAVE_NAME = \"CNN_v1\"\n",
        "\n",
        "# Evaluate Model\n",
        "evaluate_status = True\n",
        "EVALUATE_NAME = SAVE_NAME\n",
        "\n",
        "# Learning Rate\n",
        "LR = 0.001\n",
        "\n",
        "# Batch Size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Number of Epochs\n",
        "EPOCHS = 10\n",
        "\n",
        "# Early Stopping Patience ( Stop training if no improvement in test loss after specificed number of epochs )\n",
        "PATIENCE = 10\n",
        "early_stop_status = True   # Set to True to turn on Early Stopping\n",
        "\n",
        "# Input Resolution Image will be resized and passed to CNN\n",
        "RESIZED_RESOLUTION = 256"
      ],
      "metadata": {
        "id": "9ks6A-6McoxZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Prerequisites\n"
      ],
      "metadata": {
        "id": "lDkFhUnVoFZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries"
      ],
      "metadata": {
        "id": "P3D-YdsToOFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "# Data Transforms\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# Dataset and Models\n",
        "from torchvision import datasets, models\n",
        "\n",
        "# Data Loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Mathplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "MQ8SQENG62Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e26d05b-0b7a-41be-e424-15409c29e36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Class Counts"
      ],
      "metadata": {
        "id": "mKpDl8-Fp8vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Paths\n",
        "data_path = Path(\"data/\")\n",
        "drive_path = Path(\"drive/MyDrive\")\n",
        "datasets_path = drive_path / \"ML Data Sets\"\n",
        "pneumonia_path = datasets_path / FOLDER_NAME\n",
        "\n",
        "def print_class_counts(dir_path, print_dir = False):\n",
        "\n",
        "    # Printing directories\n",
        "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "        if print_dir:\n",
        "            print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "jQvMuee2wWHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataTransforms(phase = None):\n",
        "\n",
        "    if phase == 'train':\n",
        "\n",
        "        data = T.Compose([\n",
        "            T.Resize(size=(256,256)),\n",
        "            T.CenterCrop(size=224),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomRotation(degrees = (-20, +20)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    elif phase == 'test' or phase == 'val':\n",
        "\n",
        "        data = T.Compose([\n",
        "\n",
        "                T.Resize(size = (224,224)),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "tO5u53vQ7gZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir = \"/content/drive/MyDrive/pneumonia/chest/chest_xray/chest_xray\"\n",
        "test = 'test'\n",
        "train = 'train'\n",
        "\n",
        "trainSet = datasets.ImageFolder(os.path.join(dataDir, train),transform = dataTransforms(train))\n",
        "testSet = datasets.ImageFolder(os.path.join(dataDir, test),transform = dataTransforms(test))"
      ],
      "metadata": {
        "id": "gqUt9s0971uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classNames = trainSet.classes\n",
        "print(\"Class Names:\", classNames)\n",
        "print(\"Class-to-Index Mapping:\", trainSet.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxS6J9U-8B-O",
        "outputId": "cc609ac1-b795-41ad-871c-06fa572fba7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Names: ['NORMAL', 'PNEUMONIA']\n",
            "Class-to-Index Mapping: {'NORMAL': 0, 'PNEUMONIA': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = np.array([1341, 3875])  # Example class frequencies\n",
        "total_samples = np.sum(class_counts)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Convert to a tensor\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "if torch.cuda.is_available():\n",
        "    class_weights = class_weights.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRJnW-TY8FAc",
        "outputId": "54d78951-e6bd-42b7-ca35-603d44347752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: [1.9448173  0.67303226]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader = DataLoader(trainSet,batch_size = 64,shuffle = True)\n",
        "testLoader = DataLoader(testSet,batch_size = 64,shuffle = False)\n",
        "\n",
        "images, labels = next(iter(trainLoader))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqG6gKJz8G1y",
        "outputId": "47fe5850-4d90-4909-cec8-5f8abe1f102f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torchvision\n",
        "resnet_model = torchvision.models.resnet50(weights = 'IMAGENET1K_V2')\n",
        "print(resnet_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOkLzP7Kh34I",
        "outputId": "39356eeb-3279-49c2-ce46-34dbd6bba95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 134MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add a global average pooling operation to match the expected input of fc layer\n",
        "        x = torch.mean(x, dim=[2, 3])  # Average over the spatial dimensions (height and width)\n",
        "        return x\n",
        "\n",
        "resnet_model.avgpool = Identity()\n",
        "resnet_model.fc = nn.Linear(in_features=2048, out_features=2)"
      ],
      "metadata": {
        "id": "GaiibUXEidPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the optimizer\n",
        "optimizer = optim.SGD(resnet_model.parameters(), lr=0.01)\n",
        "\n",
        "# defining the loss function\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "# defining the scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "bJpzDCPpveHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    resnet_model.to(\"cuda\")\n",
        "\n",
        "Losses = []\n",
        "for i in range(10):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainLoader:\n",
        "\n",
        "        # Changing images to cuda for GPU\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Training pass\n",
        "        # Sets the gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = resnet_model(images)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # This is where the model learns by backpropagating\n",
        "        # Accumulates the loss for mini batch\n",
        "        loss.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        Losses.append(loss.item())\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Scheduler step after each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"Epoch {} - Training loss: {}\".format(i + 1, running_loss / len(trainLoader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UdYtpDZmQ7U",
        "outputId": "dce12e9c-b351-46d7-fdd9-7ff0311da88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training loss: 0.4879041761159897\n",
            "Epoch 2 - Training loss: 0.26548882573843\n",
            "Epoch 3 - Training loss: 0.18024961115873378\n",
            "Epoch 4 - Training loss: 0.1607732931556909\n",
            "Epoch 5 - Training loss: 0.13397014626990195\n",
            "Epoch 6 - Training loss: 0.12977995213283144\n",
            "Epoch 7 - Training loss: 0.11777928524205218\n",
            "Epoch 8 - Training loss: 0.11754151628069255\n",
            "Epoch 9 - Training loss: 0.11833742422902066\n",
            "Epoch 10 - Training loss: 0.11851817418051802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3tIhsEK4-67",
        "outputId": "03cb25c9-d0be-43f5-ed54-9de3745d4a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/868.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/868.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m593.9/868.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        " # Getting all test predictions\n",
        "y_preds = []\n",
        "y_preds_pre_argmax = []\n",
        "y_labels = []\n",
        "transfer_model.eval()\n",
        "with torch.inference_mode():\n",
        "  for (X, y) in (testLoader):\n",
        "    X, y = X.to('cuda'), y.to('cuda')\n",
        "    y_pred = transfer_model(X)\n",
        "    y_preds_pre_argmax.append(y_pred)\n",
        "    y_preds.append(y_pred.argmax(dim = 1).cpu())\n",
        "    y_labels.append(y)\n",
        "  # Converting list to tensor ( y_preds has shape ([624]) = len(dataLoader) )\n",
        "y_preds_tensor = torch.cat(y_preds).cpu()\n",
        "y_preds_pre_argmax_tensor = torch.cat(y_preds_pre_argmax).cpu()\n",
        "  # Converting test_data lables to tensor ( labels_tensor has shape ([624]) )\n",
        "labels_tensor = torch.tensor(test_data.targets).cpu()\n",
        "\n",
        "\n",
        "  # AUROC\n",
        "auroc_obj = torchmetrics.classification.MulticlassAUROC(num_classes = 2, average = None, thresholds = None)\n",
        "auroc = auroc_obj(y_preds_pre_argmax_tensor, labels_tensor)\n",
        "print(f\"AUROC for NORMAL (Class 0): {auroc[0]:0.3f}\")\n",
        "print(f\"AUROC for PNEUMONIA (Class 1): {auroc[1]:0.3f}\")"
      ],
      "metadata": {
        "id": "oy6Q21lr4D1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "# Creating models directory\n",
        "MODEL_NAME = \"transfer_model\"\n",
        "MODEL_PATH = Path(\"drive/MyDrive/ML Models\")\n",
        "if not MODEL_PATH.is_dir():\n",
        "  MODEL_PATH.mkdir(parents = True, exist_ok = True)\n",
        "else:\n",
        "  print(\"Model save path (drive/MyDrive/ML Models) already exists.\")\n",
        "# Creating save path\n",
        "MODEL_NAME = MODEL_NAME + \".pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "# Saving model state_dict\n",
        "torch.save(obj = transfer_model.state_dict(), f = MODEL_SAVE_PATH)\n",
        "print(f\"Model Saved as {MODEL_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S53S2fMm5-EJ",
        "outputId": "c8f619fc-6160-4ca3-8698-0095ce33469b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved as transfer_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize lists to store true labels and predicted probabilities\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "resnet_model.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for data in testLoader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        outputs = resnet_model(images)\n",
        "\n",
        "        # Assuming outputs are raw scores (logits)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        # Store the true labels and predicted probabilities\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "# Compute AUROC for each class\n",
        "auroc_per_class = []\n",
        "for i in range(all_probs.shape[1]):\n",
        "    auroc = roc_auc_score(all_labels == i, all_probs[:, i])\n",
        "    auroc_per_class.append(auroc)\n",
        "\n",
        "# Compute the average AUROC\n",
        "average_auroc = np.mean(auroc_per_class)\n",
        "\n",
        "print(f'AUROC of class NORMAL: {auroc_per_class[0]}')\n",
        "print(f'AUROC of class PNEUMONIA: {auroc_per_class[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxnsauXT3Qx8",
        "outputId": "49f72574-4681-45f9-8d7c-d42dda3c4036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC of class NORMAL: 0.9347797501643655\n",
            "AUROC of class PNEUMONIA: 0.9347797501643655\n",
            "Average AUROC: 0.9347797501643655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class classify(nn.Module):\n",
        "    def __init__(self,num_classes=2):\n",
        "        super(classify,self).__init__()\n",
        "\n",
        "        self.conv1=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
        "        self.relu1=nn.ReLU()\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
        "        self.relu2=nn.ReLU()\n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
        "        self.relu3=nn.ReLU()\n",
        "        self.fc=nn.Linear(in_features=32 * 112 * 112,out_features=num_classes)\n",
        "\n",
        "\n",
        "    def forward(self,input):\n",
        "        output=self.conv1(input)\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu1(output)\n",
        "        output=self.pool(output)\n",
        "        output=self.conv2(output)\n",
        "        output=self.relu2(output)\n",
        "        output=self.conv3(output)\n",
        "        output=self.bn3(output)\n",
        "        output=self.relu3(output)\n",
        "        output=output.view(-1,32*112*112)\n",
        "        output=self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "JQ6uazqj8L18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the optimizer\n",
        "optimizer = optim.Adam(transfer_model.parameters(), lr=0.01)\n",
        "# defining the loss function\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "eU6wQlYa8tFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Losses = []\n",
        "for i in range(10):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainLoader:\n",
        "\n",
        "        #Changing images to cuda for gpu\n",
        "        if torch.cuda.is_available():\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # Training pass\n",
        "        # Sets the gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #This is where the model learns by backpropagating\n",
        "        # accumulates the loss for mini batch\n",
        "        loss.backward()\n",
        "\n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        Losses.append(loss)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {}\".format(i+1, running_loss/len(trainLoader)))"
      ],
      "metadata": {
        "id": "JFhdYI8Z8v7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "9356e9a5-6271-4cde-ded8-ba3bfd842a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fbc3985d5787>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count, all_count = 0, 0\n",
        "for images,labels in testLoader:\n",
        "  for i in range(len(labels)):\n",
        "    if torch.cuda.is_available():\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "    img = images[i].view(1, 3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "        logps = resnet_model(img)\n",
        "\n",
        "\n",
        "    ps = torch.exp(logps)\n",
        "    probab = list(ps.cpu()[0])\n",
        "    pred_label = probab.index(max(probab))\n",
        "    true_label = labels.cpu()[i]\n",
        "    if(true_label == pred_label):\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "metadata": {
        "id": "_zT_rqz38y2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a220a5fb-0771-48af-b16f-2c1cbc3c0c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of Images Tested = 624\n",
            "\n",
            "Model Accuracy = 0.8830128205128205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AODFDmiwuLfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}